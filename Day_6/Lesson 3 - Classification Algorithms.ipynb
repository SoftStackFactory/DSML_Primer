{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:42px; text-align:center; margin-bottom:30px;\"><span style=\"color:SteelBlue\">Lesson 3:</span> Classification Algorithms</h1>\n",
    "<hr>\n",
    "\n",
    "Welcome to <span style=\"color:royalblue\">Lesson 3: Classification Algorithms</span>!\n",
    "\n",
    "In this lesson, we'll dive into a few more key concepts for machine learning. In particular, we want to introduce you to 4 algorithms:\n",
    "1. $L_1$-regularized logistic regression\n",
    "2. $L_2$-regularized logistic regression\n",
    "3. Random forests\n",
    "4. Boosted trees\n",
    "\n",
    "Just as in the previous project, we'll provide a gentle introduction to the **intuition and practical benefits** of each algorithm.\n",
    "\n",
    "<br><hr id=\"toc\">\n",
    "\n",
    "### In this lesson...\n",
    "\n",
    "In this lesson we'll walk through more key machine learning concepts, plus 4 effective algorithms for classification tasks.\n",
    "\n",
    "1. [Binary classification](#binary)\n",
    "2. [Toy example: noisy conditional](#conditional)\n",
    "3. [Logistic Regression](#logistic)\n",
    "3. [Regularized logistic algorithms](#regularized-logistic) - $L_1$-regularized and $L_2$-regularized\n",
    "4. [Tree ensemble algorithms](#tree-ensembles) - Random Forests and Boosted Trees\n",
    "\n",
    "**Tip:** Each section builds on the previous ones.\n",
    "\n",
    "<br><hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, let's import libraries that we'll need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NumPy and Pandas\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "# Matplotlib, and remember to display plots in the notebook\n",
    "from matplotlib import pyplot as plt \n",
    "%matplotlib inline\n",
    "\n",
    "# Seaborn for easier visualization\n",
    "import seaborn as sns "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span id=\"binary\"></span>\n",
    "# 1. Binary classification\n",
    "\n",
    "Classification with 2 classes is so common that it gets its own name: **binary classification.** \n",
    "\n",
    "\n",
    "Just to be clear, let's take another look at the **target variable** for this problem.  First, let's look at it in the raw dataset (before we created the analytical base table)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print unique classes for 'status' and the first 5 observations for 'status' in the raw dataset\n",
    "raw_df = pd.read_csv('project_files/clean_employee_data.csv')\n",
    "\n",
    "print(raw_df.status.unique())\n",
    "raw_df.status.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, when we constructed our analytical base table, we converted the target variable from <code style=\"color:crimson\">'Left' / 'Employed'</code> into <code style=\"color:crimson\">1 / 0</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print unique classes for 'status' and the first 5 observations for 'status' in the analytical base table\n",
    "abt_df = pd.read_csv('project_files/employee_analytical_base_table.csv')\n",
    "\n",
    "print(abt_df.status.unique())\n",
    "abt_df.status.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which is the **positive** class? How about the **negative** class?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center; margin: 40px 0 40px 0; font-weight:bold;\">\n",
    "[Back to Contents](#toc)\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span id=\"conditional\"></span>\n",
    "# 2 - Toy example: noisy conditional\n",
    "\n",
    "We're going to use another toy example, just as we did in Project 1. \n",
    "\n",
    "This time, we're going to build models for a **noisy conditional**.\n",
    "\n",
    "\n",
    "Let's create that dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input feature\n",
    "x = np.linspace(0, 1, 100)\n",
    "# Noise\n",
    "np.random.seed(555)\n",
    "noise = np.random.uniform(-0.2, 0.2, 100)\n",
    "\n",
    "# Target variable\n",
    "y = ((x + noise) > 0.5).astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to **reshape** <code style=\"color:steelblue\">x</code> before moving on.\n",
    "* That's because Scikit-Learn algorithms expect input features with 2 axes. However, right now, <code style=\"color:steelblue\">x</code> only has one.\n",
    "\n",
    "To make sure it has 2 axes, reshape it to be (100, 1) and name the the reshaped object capital <code style=\"color:steelblue\">X</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape x into X\n",
    "X = x.reshape(100, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, plot a **scatterplot** of the synthetic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot scatterplot of synthetic dataset\n",
    "plt.scatter(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center; margin: 40px 0 40px 0; font-weight:bold;\">\n",
    "[Back to Contents](#toc)\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span id=\"logistic\"></span>\n",
    "# 3. Logistic regression\n",
    "\n",
    "First, we'll discuss **logistic regression**, which is the classification analog of linear regression.\n",
    "\n",
    "Let's actually fit a linear regression model first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import LinearRegression and LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit a linear model, make predictions, and plot them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear model\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Plot dataset and predictions\n",
    "plt.scatter(X, y)\n",
    "plt.plot(X, model.predict(X), 'k--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's see how **logistic regression** differs.\n",
    "\n",
    "Let's fit a logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression\n",
    "model = LogisticRegression()\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's call the <code style=\"color:steelblue\">.predict()</code> function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict()\n",
    "model.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call <code style=\"color:steelblue\">.predict_proba()</code> on the first 10 observations and display the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict_proba()\n",
    "pred = model.predict_proba(X[:10])\n",
    "\n",
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the predictions for the first observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class probabilities for first observation\n",
    "pred[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the probability of **just the positive class** for the first observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positive class probability for first observation\n",
    "pred[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a simple list comprehension to extract a **list of only the predictions for the positive class**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just get the second value for each prediction\n",
    "pred = [pred[1] for p in pred]\n",
    "\n",
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, let's fit and plot the logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression\n",
    "model = LogisticRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Predict probabilities\n",
    "pred = model.predict_proba(X)\n",
    "\n",
    "# Just get the second value (positive class) for each prediction\n",
    "pred = [p[1] for p in pred]\n",
    "\n",
    "# Plot dataset and predictions\n",
    "plt.scatter(X, y)\n",
    "plt.plot(X, pred, 'k--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center; margin: 40px 0 40px 0; font-weight:bold;\">\n",
    "[Back to Contents](#toc)\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span id=\"regularized-logistic\"></span>\n",
    "# 4. Regularized logistic regression\n",
    "\n",
    "Logistic regression has regularized versions that are analogous to those for linear regression.\n",
    "\n",
    "Just to save ourselves from repeating the same code, let's write a quick helper function that:\n",
    "1. Fits any classification model\n",
    "2. Makes predictions\n",
    "3. Extracts the positive probabilities\n",
    "4. Plots them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_plot_classifier(clf):\n",
    "    # Fit model\n",
    "    clf.fit(X, y)\n",
    "    \n",
    "    # Predict and take second value of each prediction\n",
    "    pred = clf.predict_proba(X)\n",
    "    pred = [p[1] for p in pred]\n",
    "    \n",
    "    # Plot\n",
    "    plt.scatter(X, y)\n",
    "    plt.plot(X, pred, 'k--')\n",
    "    plt.show()\n",
    "    \n",
    "    # Return fitted model and predictions\n",
    "    return clf, pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit and plot the same logistic regression from earlier, this time using <code style=\"color:steelblue\">fit_and_plot_classifier()</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression\n",
    "clf, pred = fit_and_plot_classifier(LogisticRegression())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the penalty **4 times stronger**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# More regularization\n",
    "# C : float, default: 1.0\n",
    "# Inverse of regularization strength; must be a positive float. \n",
    "# Like in support vector machines, smaller values specify stronger regularization.\n",
    "clf, pred = fit_and_plot_classifier(LogisticRegression(C=0.25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, make the penalty **4 times weaker**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Less regularization\n",
    "clf, pred = fit_and_plot_classifier(LogisticRegression(C=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To basically remove regularization, bump <code style=\"color:steelblue\">C</code> way up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basically no regularization\n",
    "clf, pred = fit_and_plot_classifier(LogisticRegression(C=10000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the **penalty type** to use $L_1$ regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L1 regularization\n",
    "clf, pred = fit_and_plot_classifier(LogisticRegression())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize $L_1$-regularized and $L_2$-regularized logistic regression **separately** and **explicitly**.\n",
    "\n",
    "**penalty** : str, ‘l1’ or ‘l2’, default: ‘l2’\n",
    "Used to specify the norm used in the penalization. The ‘newton-cg’, ‘sag’ and ‘lbfgs’ solvers support only l2 penalties.\n",
    "\n",
    "**random_state** : int, RandomState instance or None, optional, default: None\n",
    "The seed of the pseudo random number generator to use when shuffling the data. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random. Used when solver == ‘sag’ or ‘liblinear’.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L1-regularized logistic regression\n",
    "l1 = LogisticRegression(penalty='l1', random_state=123)\n",
    "\n",
    "# L2-regularized logistic regression\n",
    "l2 = LogisticRegression(penalty='12', random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, use $L_1$-regularization with a 4 times weaker penalty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L1 regularization with weaker penalty\n",
    "clf, pred = fit_and_plot_classifier(LogisticRegression(penalty='l1', C=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center; margin: 40px 0 40px 0; font-weight:bold;\">\n",
    "[Back to Contents](#toc)\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span id=\"tree-ensembles\"></span>\n",
    "# 5. Tree ensemble algorithms\n",
    "\n",
    "The same tree ensembles we used for regression can be applied to classification. \n",
    "\n",
    "First, import the random forest classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply it to this toy problem.\n",
    "\n",
    "**n_estimators** : integer, optional (default=10)\n",
    "The number of trees in the forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest classifier\n",
    "clf, pred = fit_and_plot_classifier(RandomForestClassifier(n_estimators=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, import the boosted tree classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import GradientBoostingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, apply it to this toy problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest classifier\n",
    "clf, pred = fit_and_plot_classifier(GradientBoostingClassifier(n_estimators=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center; margin: 40px 0 40px 0; font-weight:bold;\">\n",
    "[Back to Contents](#toc)\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Next Steps\n",
    "\n",
    "Alright, that was a nice tour through some key theory and concepts, but let's get ready to dive back into the project!\n",
    "\n",
    "As a reminder, here are a few things you did in this module:\n",
    "* You learned some key terminology for binary classification, such as \"positive\" vs. \"negative\" classes.\n",
    "* You saw how logistic regression can also be regularized.\n",
    "* You played around with different settings for penalty strength.\n",
    "* And you recruited 4 algorithms: $L_1$-Regularized Logistic, $L_2$-Regularized Logistic, Random Forests, and Boosted Trees.\n",
    "\n",
    "\n",
    "<p style=\"text-align:center; margin: 40px 0 40px 0; font-weight:bold;\">\n",
    "[Back to Contents](#toc)\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
